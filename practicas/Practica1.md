
# Introducción a la inteligencia artificial

## Introducción
Desde el comienzo de los tiempos, la inteligencia humana ha sido de gran importancia para los hombres, siempre estamos tratando de entender como funcionan nuestras ideas y pensamientos y si es posible elevar nuestra inteligencia, en base a cómo funciona nuestro cerebro y que tanto podemos hacer con esta inteligencia pero, desde hace algún tiempo hasta la fecha, se comenzó a pensar en la posibilidad de que un agente externo, es decir, un agente computacional piense por si mismo o tenga esta `Inteligencia`, para resolver algunas tareas específicas o de síntesis y automatización de tareas, todo esto, simulando como funcionamos nosotros los humanos.
Entonces, como se mencionó, la IA pretende crear un agente que resuelva tareas, según los distintos conceptos que se tienen, algunos dicen que puede ser a base de razonamiento, es decir, cálculos matemáticos e ingeniería, o imitando el funcionamiento humano, que trabaja como una ciencia empírica a prueba y error.

### Comportamiento humano

#### Prueba de Turin

`La Prueba de Turing`, propuesta por Alan Turing (1950), propone una prueba más fácil que hacer una lista de cualidades a tachar para saber si un agente es inteligente o no, esta prueba se basa en una serie de preguntas o una conversación entre un computador y un humano, la prueba se basa en que el humano no pueda distinguir si está hablando con un humano o con un computador, como es en realidad.
Hoy en día, sabemos toda la tecnología que se requiere para realizar esta prueba:

* Procesamiento de lenguaje natural que le permita comunicarse satisfactoriamente en el idioma del evaluador, ya que el agente y el evaluador no hablan el mismo idioma.
* Representación del conocimiento para almacenar lo que se conoce o siente. 
* Razonamiento automático para utilizar la información almacenada para responder a preguntas y extraer nuevas conclusiones. 
* Aprendizaje automático para adaptarse a nuevas circunstancias y para detectar y extrapolar patrones
* Visión computacional para percibir objetos. 
* Robótica para manipular y mover objetos.



####  Enfoque moderno cognitivo 

Para poder hacer que un computador funcione como humano, se requiere saber primero como piensan los humanos.
Crear una teoría en base a introspección o experimentos/pruebas psicológicas, para aplicar esta teoría en el funcionamiento del agente.
En el campo interdisciplinario de la `ciencia cognitiva` convergen modelos computacionales de IA y técnicas experimentales de psicología intentando elaborar teorías precisas y verificables sobre el funcionamiento de la mente humana.

#### El enfoque de las «leyes del pensamiento

El filosofo Aristóteles, propuso que hay una manera correcta de pensar, es decir, que todo pensamiento está dado en base a los hechos lógicos, sigue leyes y reglas, no es casualidad, sino que todo está dado a base de acciones lógicas.
En base a este pensamiento, se desarrollaron métodos lógicos con notación lógica para resolver problemas, pero, esto tiene algunos problemas, no todas las entradas pueden ser fácilmente cambiadas a esta notación para ser resueltas y, a veces al llevar a la practica no es lo mismo que en el entrenamiento.

#### El enfoque del agente racional

Un agente racional se distingue de los demás programas porque, además de razonar debe tener otras características, como poder tomar decisiones autónomas y acoplarse al entorno, la definición formal dice que el agente racional es aquel que alcanza el mejor resultado o el mejor  resultado posible.
Se dice que esto puede ser logrado a partir del `racionalismo`, por medio de `inferencias`, pero estas no lo son todo, algunos actos del pensamiento humano son solo reflejos aprendidos por la experiencia. Es por ello, que es más importante dotar al agente con buenas herramientas para que “vea” o analice su entorno de la mejor manera posible y, los problemas que no se puedan resolver con inferencia poder buscar o aprender del entorno la mejor manera de resolver el problema, este enfoque se basa más en diseñar bien al agente.

## Los fundamentos de la Inteligencia Artificial

Como ya sabemos, las teorías de cómo hacer que funcione correctamente un agente se basaron primero  en razonamiento, hacerlo funcionar por medio de leyes, `leyes del pensamiento`, posteriormente hubo varios filósofos que opinaron al respecto, por ejemplo Rene Descartes, quien fue un defensor del `dualismo`,  creyendo que, si bien muchas cosas están dadas por las leyes físicas, como, la posición en la que caerá una piedra al ser arrojada con cierta velocidad y a cierta distancia, y una parte del alma, la cual no se rige por ninguna ley.
En cambio, hay otras corrientes como lo son el `materialismo`, que dice que todo lo que hacemos está dado con las opciones que la física nos da y el “libre albedrío” es solo escoger una de estas opciones.
El movimiento `empírico`, iniciado con el Novum Organum , de Francis Bacon (1561-1626), dice que nada existe en la mente que no haya pasado antes por los sentidos, es decir, que todo se aprende, hacemos una acción, vemos el resultado y a la siguiente vez, si el resultado no fue el correcto, cambiaremos alguna variable con el objetivo de hacer que el resultado mejore.

### Matemáticas

Los filósofos formularon algunas teorías para saber que reglas formales se debían calcular para llegar a resultados validos y aprender a razonar correctamente.
Se comenzó por la creación de la lógica proposicional o booleana, posteriormente esta se extendió a los objetos y relaciones, los objetos computacionales, que eran una representación de los objetos de la vida real, pero en programación.
Después del desarrollo del primer algoritmo para calcular el máximo común divisor, se propuso que, en realidad no existe un algoritmo que determine la validez de cualquier proposición lógica al 100 %, que siempre habrá un sesgo, de ahí el `teorema de la incompletitud` de Kurt Gödel, en el que se demostró que en cualquier lenguaje que tuviera la capacidad suficiente para expresar las propiedades de los números naturales, existen aseveraciones verdaderas no decidible en el sentido de que no es posible decidir su validez mediante ningún algoritmo.
El resultado fundamental anterior se puede interpretar también como la indicación de que existen algunas funciones de los números enteros que no se pueden representar mediante un algoritmo, es decir no se pueden calcular.
Además de separar las funciones que sí puedan ser computadas  de las que no, se tenia el problema de la `intratabilidad`, que se da cuando un problema es tan grande que no se puede resolver, la solución es separar el problema en partes pequeñas de este que puedan ser resultas y tratables.


### Neurociencia

La neurociencia es el estudio del sistema neurológico y en especial el cerebro.
El estudio de Paul Broca (1824-1880) sobre la afasia (dificultad para hablar) en pacientes con el cerebro dañado, en 1861, le dio fuerza a este campo y convenció a la sociedad médica de la existencia de áreas localizadas en el cerebro responsables de funciones cognitivas específicas. En particular, mostró que la producción del habla se localizaba en una parte del hemisferio izquierdo; hoy en día conocida como el área de Broca7 . En esta época ya se sabía que el cerebro estaba formado por células nerviosas o neuronas, pero no fue hasta 1873 cuando Camillo Golgi (1843-1926) desarrolló una técnica de coloración que permitió la observación de `neuronas` individuales en el cerebro.
En la actualidad se dispone de información sobre la relación existente entre las áreas del cerebro y las partes del cuerpo humano que controlan o de las que reciben impulsos sensoriales.
Se concluye que un conjunto de células es capaz de llegar al razonamiento, acción y conciencia.
La conceptualización del cerebro como un dispositivo de procesamiento de información, característica principal de la psicología cognitiva, se remonta por lo menos a las obras de William James10 (1842-1910). Helmholtz también pone énfasis en que la percepción entraña cierto tipo de inferencia lógica inconsciente.
Kenneth Craik (1943) en su obra The Nature of Explanation establece tres elementos clave que hay que tener en cuenta para diseñar un agente basado en conocimiento:

* El estímulo deberá ser traducido a una representación interna.
* La representación se debe manipular mediante procesos cognitivos para así generar nuevas representaciones internas.
* Éstas, a su vez, se traducirán de nuevo en acciones.

### Ingeniería computacional 

Para la creación de la IA se requiere inteligencia y un artefacto, que estarán dados por el hardware, el cual es cada vez más eficiente y se le van agregando más cosas para que funcione más rápido y mejor, y el software.
La IA también tiene una deuda con la parte software de la informática que ha proporcionado los sistemas operativos, los lenguajes de programación, y las herramientas necesarias para escribir programas modernos (y artículos sobre ellos). Sin embargo, en esta área la deuda se ha saldado: la investigación en IA ha generado numerosas ideas novedosas de las que se ha beneficiado la informática en general, como por ejemplo el tiempo compartido, los intérpretes imperativos, los computadores personales con interfaces gráficas y ratones, entornos de desarrollo rápido, listas enlazadas, administración automática de memoria, y conceptos claves de la programación simbólica, funcional, dinámica y orientada a objetos.

## Nacimiento de la inteligencia artificial

Los primeros años de la IA estuvieron llenos de éxito. Teniendo en cuenta lo primitivo de los computadores y las herramientas de programación de aquella época, y el hecho de que sólo unos pocos años antes, a los computado res se les consideraba como artefactos que podían realizar trabajos aritméticos y nada más, resultó sorprendente que un computador hiciese algo remotamente inteligente. La comunidad científica, en su mayoría, prefirió creer que una máquina nunca podría hacer tareas. Naturalmente, los investigadores de IA responderían demostrando la realización de una tarea tras otra. John McCarthy se refiere a esta época como la era de ¡Mira, mamá, ahora sin manos!. 
El éxito del SRGP y de los programas que le siguieron, como los modelos de cognición, llevaron a Newell y Simon (1976) a formular la famosa hipótesis del sistema de `símbolos físicos`, que afirma que «un sistema de símbolos físicos tiene los medios suficientes y necesarios para generar una acción inteligente». Lo que ellos querían decir es que cualquier sistema (humano o máquina) que exhibiese inteligencia debería operar manipulando estructuras de datos compuestas por símbolos. 
En IBM, Nathaniel Rochester y sus colegas desarrollaron algunos de los primeros programas de IA. Herbert Gelernter (1959) construyó el demostrador de teoremas de geometría (DTG), el cual era capaz de probar teoremas que muchos estudiantes de matemáticas podían encontrar muy complejos de resolver. A comienzos 1952, Arthur Samuel escribió una serie de programas para el juego de las damas que eventualmente aprendieron a jugar hasta alcanzar un nivel equivalente al de un amateur.
John McCarthy se trasladó de Darmouth al MIT, donde realizó tres contribuciones cru ciales en un año histórico: 1958. En el Laboratorio de IA del MIT Memo Número 1, McCarthy definió el lenguaje de alto nivel Lisp, que se convertiría en el lenguaje de pro gramación dominante en la IA.
Al igual que el Teórico Lógico y el Demostrador de Teoremas de Geometría, McCarthy diseñó su programa para buscar la solución a problemas utilizando el conocimiento. El programa se diseñó para que aceptase nuevos axiomas durante el curso normal de operación, permitiéndole así ser com petente en áreas nuevas, sin necesidad de reprogramación. Minsky supervisó el trabajo de una serie de estudiantes que eligieron un número de problemas limitados cuya solución pareció requerir inteligencia. Estos dominios limitados se conocen como `micromundos`.
El programa SAINT de James Slagle (1963a) fue capaz de resolver problemas de integración de cálculo en forma cerrada, habituales en los primeros cursos de licenciatura. El programa ANALOGY de Tom Evans (1968) resolvía problemas de analogía geométrica que se aplicaban en las pruebas de medición de inteligencia. El programa STUDENT de Daniel Bobrow (1967) podía resolver problemas de álgebra del tipo: 
 `Si el número de clientes de Tom es dos veces el cuadrado del 20 por ciento de la cantidad de anuncios que realiza, y éstos ascienden a 45, ¿cuántos clientes tiene Tom? `

### Una dosis de realidad

Por el éxito en problemas sencillos que tuvieron los primeros agentes, se tenía mucha fe en el futuro de la Inteligencia Artificial, al grado de pensarse que en el lapso de 10 años una maquina sería capaz de ganar un juego de ajedrez y de demostrar un teorema matemático. Esto solo se cumplió en 40 años en vez de 10 como se tenía planeado, ya que, con el paso del tiempo se dieron cuenta que, los primeros agentes tuvieron éxito en problemas pequeños de una sola índole y comenzaron a fallar en problemas más grandes y variados.
Se dieron cuenta de que las maquinas tenían mucho sesgo cognitivo aún cuando, por ejemplo, se intentó hacer la traducción automática de un idioma a otro, pero, esto no era solo el remplazo de palabras de un idioma por las del otro, sino que, la maquina tenía que tener conocimiento sobre el contexto de la frase y otras cosas, y arrojaba una traducción vaga y sin sentido.
El segundo problema fue que muchos de los problemas que se estaban intentando resolver mediante la IA eran intratables.

### Sistemas basados en conocimiento

El cuadro que dibujaba la resolución de problemas durante la primera década de la investigación en la IA estaba centrado en el desarrollo de mecanismos de búsqueda de propósito general, en los que se entrelazaban elementos de razonamiento básicos para encontrar así soluciones completas. A estos procedimientos se les ha denominado `métodos débiles`, debido a que no tratan problemas más amplios o más complejos. La alternativa a los métodos débiles es el uso de conocimiento específico del dominio que facilita el desarrollo de etapas de razonamiento más largas, pudiéndose así resolver casos recurrentes en dominios de conocimiento restringido, a este tipo de sistemas se les llamó `sistemas expertos`.
Estos programas se usaron para un sinfín de cosas, como el diagnostico médico, pero también se comenzó a ahondar en los programas que procesaban el lenguaje natural, se desarrollaron varios lenguajes que con distinto razonamiento.
Algunos basados en la lógica, por ejemplo el lenguaje Prolog gozó de mucha aceptación en Europa, aceptación que en Estados Unidos fue para la familia del PLANNER. Otros, siguiendo la noción de `marcos` de Minsky (1975), se decidieron por un enfoque más estructurado, al recopilar información sobre objetos concretos y tipos de eventos, organizando estos tipos en grandes jerarquías taxonómicas, similares a las biológicas.

## Agentes racionales

Como ya se había planteado, un `agente` es cualquier cosa que percibe su entorno por medio de sensores y actuadores, la parte de `racional` esta dada por el hecho de que encuentra la mejor solución posible a un problema dado.
El término `percepción` se utiliza en este contexto para indicar que el agente puede recibir entradas en cualquier instante. La secuencia de percepciones de un agente refleja el historial completo de lo que el agente ha recibido. En general, un agente tomará una decisión en un momento dado dependiendo de la `secuencia completa de percepciones` hasta ese instante. Si se puede especificar qué decisión tomará un agente para cada una de las posibles secuencias de percepciones, entonces se habrá explicado más o menos todo lo que se puede decir de un agente.
En otras palabras, la secuencia de percepción se refiere a los pasos o las posibles acciones a seguir para un agente en un caso específico.

### Medida de rendimiento.

La `funcion del agente` es, en pocas palabras, la acción que este deberá hacer con ciertas condiciones específicas, por ejemplo, en el caso de una aspiradora que puede moverse, aspirar y no hacer nada, tendrá la condición de limpiar si el cuadrante en el que está se encuentra sucio, de lo contrario, moverse en el orden de una secuencia de percepción dada.
La `medida de rendimiento` tiene que ver con el buen comportamiento del agente, es decir, que tan eficiente es.
Cuando se sitúa un agente en un medio, éste genera una secuencia de acciones de acuerdo con las percepciones que recibe. Esta secuencia de acciones hace que su hábitat pase por una secuencia de estados. Si la secuencia es la deseada, entonces el agente habrá actuado correctamente. Obviamente, no hay una única medida adecuada para todos los agentes. 
Con el mismo ejemplo de la aspiradora, por ejemplo, si la medida de rendimiento fuera la cantidad de basura recogida en un periodo de tiempo, el agente podría tirar y recoger la basura hasta que se cumpla el plazo, de esta manera estaría satisfaciendo la medida de rendimiento, que era recoger más basura, pero esto no sería correcto. En cambio, si la medida de rendimiento fuera tener el suelo limpio, el agente podría ganar puntos por limpiar una cuadricula en cierto tiempo y tal vez, penalización por gastar más electricidad o hacer más ruido.


## Inteligencia artificial (Documental)
